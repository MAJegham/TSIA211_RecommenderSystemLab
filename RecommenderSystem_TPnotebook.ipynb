{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1.  Systèmes de recommandation\n",
    "\n",
    "\n",
    "# SD-TSIA 211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Présentation du modèle\n",
    "### Question 1.1\n",
    "\n",
    "L'importation des données nécéssite une connexion internet et prend un peu de temps (~15sec)\n",
    "Vous pourrez changer la valeur assignée à la variable \"file_path\" si vous disposez déjà du fichier u.data et commenter les 4 lignes qui le précède"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import movielensutils as mlu\n",
    "import scipy.sparse.linalg as la     # pour l'initialisation de Q0 ou P0\n",
    "import numpy as np\n",
    "from time import time \n",
    "import requests, zipfile, io\n",
    "\n",
    "#Importing the data file\n",
    "#takes some time to import the dataset\n",
    "zip_url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "zip_file = requests.get(zip_url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(zip_file.content))\n",
    "zip_file.extractall()\n",
    "\n",
    "file_path = \"ml-100k/u.data\"\n",
    "\n",
    "R, mask = mlu.load_movielens(file_path, minidata=False)\n",
    "np.shape(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 200)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1, mask1 = mlu.load_movielens(file_path, minidata=True)\n",
    "np.shape(R1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si minidata = True ; On préfère prendre une matrice de données de dimension réduite à 100x200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Les données contiennent:\n",
    "    - 943 utilisateurs\n",
    "    - 1682 utilisateurs\n",
    "    - 100000 notes disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(R))\n",
    "print(np.shape(mask[mask == True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 1.3\n",
    "**1.3.1.  Convexité ** \n",
    "\n",
    "$f(P,Q) = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-QP)||_{F}^2 + \\frac{\\rho}{2}||Q||_{F}^2+ \\frac{\\rho}{2}||P||_{F}^2 $\n",
    "\n",
    "\n",
    "Au lieu d'étudier la convéxité de la fonction f(P,Q) pour P et Q deux matrices, on réduira le problème à l'étude de la fonction \n",
    "$f(p,q) = \\frac{1}{2} ||\\mathbb{\\alpha}_{K}\\circ(r-qp)||_{F}^2 + \\frac{\\rho}{2}||q||_{F}^2+ \\frac{\\rho}{2}||p||_{F}^2 $\n",
    "où r,p et q sont des réels.\n",
    "\n",
    "$f(p,q) = \\frac{1}{2} ||\\mathbb{\\alpha}_{K}\\circ(r-qp)||_{F}^2 + \\frac{\\rho}{2}||q||_{F}^2+ \\frac{\\rho}{2}||p||_{F}^2 $\n",
    "$\\quad = \\frac{1}{2} (r-qp)^2 + \\frac{\\rho}{2}q^2+ \\frac{\\rho}{2}p^2 $\n",
    "$\\quad = \\frac{1}{2} r^2 + \\frac{1}{2} q^2p^2  - rqp + \\frac{\\rho}{2}q^2+ \\frac{\\rho}{2}p^2 $\n",
    "\n",
    "$\\nabla f(p,q) = [\\quad q²p + \\rho p - rq\\quad ,\\quad p²q - rq +\\rho q \\quad]^T$\n",
    "\n",
    "$Hess f(p,q) =\n",
    "  \\begin{bmatrix}\n",
    "    q²+\\rho & 2pq-r \\\\\n",
    "    2pq-r & p^2+\\rho\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Pour p=q=r; $\\quad det(Hess f(0,0)) = det( \\begin{bmatrix} \\rho & -r \\\\ -r & \\rho \\end{bmatrix} )  = \\rho^2-r²$\n",
    "\n",
    "$Si \\quad r^2 > \\rho^2, \\qquad det(Hess f(0,0)) < 0 \\quad \\Rightarrow \\quad$ La hessienne ne peut avoir que des valeurs propres positives.\n",
    "\n",
    "$\\Rightarrow \\quad Hess f(0,0)$ ne peut être définie positive dans ce cas\n",
    "\n",
    "Donc, $f(p,q)$ n'est pas convexe\n",
    "\n",
    "Finalement, $f(P,Q)$ n'est pas convexe\n",
    "\n",
    "\n",
    "**1.3.2.  Calcul du gradient de la fonction f**\n",
    "\n",
    "\n",
    "$f(P,Q)=\\frac{1}{2}\\sum\\limits_{(u,i)\\in K} (R_{u,i}-\\sum\\limits_{c \\in C} Q_{u,c}P_{c,i})^2 + \\frac{\\rho}{2}\\sum\\limits_{u\\in U, c\\in C} Q_{u,c}^2 + \\frac{\\rho}{2} \\sum\\limits_{i\\in I, c\\in C} P_{c,i}^2$\n",
    "\n",
    "Soient:\n",
    "\n",
    "$f_{1}(P,Q)= \\frac{1}{2}\\sum\\limits_{(u,i)\\in K} (R_{u,i}-\\sum\\limits_{c \\in C} Q_{u,c}P_{c,i})^2 $\n",
    "\n",
    "$f_{2}(P,Q)= \\frac{\\rho}{2}\\sum\\limits_{u\\in U, c\\in C} Q_{u,c}^2 $\n",
    "\n",
    "$f_{3}(P,Q)= \\frac{\\rho}{2} \\sum\\limits_{i\\in I, c\\in C} P_{c,i}^2 $\n",
    "\n",
    "$\\bullet \\frac{\\partial f_{1}}{\\partial P_{l,m}} \\quad = \\sum\\limits_{i \\in I, u \\in U} (\\mathbb{1}_{K})_{i,u} \t\\circ (R_{i,u}-\\sum\\limits_{c \\in C} Q_{i,c}P_{c,i})(-\\sum\\limits_{c \\in C} Q_{i,c} \\delta_{l,c} \\delta_{m,u})$\n",
    "\n",
    "$ \\quad \\quad \\quad  = \\sum\\limits_{i \\in I} (\\mathbb{1}_{K})_{i,m} \t\\circ (R_{i,m}-\\sum\\limits_{c \\in C} Q_{i,c}P_{c,m})(- Q_{i,l})$\n",
    "\n",
    "$\\quad \\quad  \\quad = - \\sum\\limits_{i \\in I} (\\mathbb{1}_{K})_{i,m} \t\\circ (R_{i,m}- (QP)_{i,m})Q_{i,l} $\n",
    "\n",
    "$\\frac{\\partial f_{1}}{\\partial P_{l,m}} \\quad = - ( Q^{t} ( \\mathbb{1}_{K} \t\\circ (R - QP)) )_{l,m}$\n",
    "\n",
    "$\\bullet \\frac{\\partial f_{2}}{\\partial P_{l,m}} \\quad = 0 $\n",
    "\n",
    "$ \\bullet \\frac{\\partial f_{3}}{\\partial P_{l,m}} \\quad = \\rho P_{l,m} $\n",
    "\n",
    "$\\Rightarrow $\n",
    "$\\nabla_{p} f (P,Q)  = -  Q^{t} ( \\mathbb{1}_{K} \t\\circ (R - QP))  + \\rho P $\n",
    "\n",
    "$\\bullet \\frac{\\partial f_{2}}{\\partial Q_{l,m}} \\quad = \\rho P_{l,m} $\n",
    "\n",
    "$ \\bullet \\frac{\\partial f_{3}}{\\partial Q_{l,m}} \\quad = 0 $\n",
    "\n",
    "$f_{1} (P, Q+H) + f_{1} (P,Q) = \\frac {1}{2} || \\mathbb{1}_{K} \t\\circ (R - (Q + H)P)||_{F} ^ 2 - \\frac {1}{2} || \\mathbb{1}_{K} \t\\circ (R - QP)||_{F} ^ 2$\n",
    "$ \\quad \\quad  \\quad  \\quad  \\quad \\quad \\quad \\quad \\quad = \\frac {1}{2} || \\mathbb{1}_{K} \t\\circ (R - QP)||_{F} ^ 2 - <\\mathbb{1}_{K} \t\\circ (R - QP),\\mathbb{1}_{K} \t\\circ HP> + \\frac {1}{2} || \\mathbb{1}_{K} \t\\circ HP||_{F} ^ 2  -\\frac {1}{2} || \\mathbb{1}_{K} \t\\circ (R - QP)||_{F} ^ 2  $\n",
    "\n",
    "$ \\quad \\quad  \\quad  \\quad  \\quad \\quad \\quad \\quad \\quad= - <\\mathbb{1}_{K} \t\\circ (R - QP),\\mathbb{1}_{K} \t\\circ HP> + o(H)$\n",
    "\n",
    "$ \\quad \\quad  \\quad  \\quad  \\quad \\quad \\quad \\quad \\quad = - <\\mathbb{1}_{K} \t\\circ (R - QP),HP> + o(H)$\n",
    "\n",
    "$ \\quad \\quad  \\quad  \\quad  \\quad \\quad \\quad \\quad \\quad= - tr ((\\mathbb{1}_{K} \t\\circ (R - QP))^{t} HP) + o(H)$\n",
    "\n",
    "$ \\quad \\quad  \\quad  \\quad  \\quad \\quad \\quad \\quad \\quad = - tr (P(\\mathbb{1}_{K} \t\\circ (R - QP))^{t} H) + o(H)$\n",
    "\n",
    "$ \\quad \\quad  \\quad  \\quad  \\quad \\quad \\quad \\quad \\quad = - <\\mathbb{1}_{K} \t\\circ (R - QP)P^{t},H> + o(H)$\n",
    "\n",
    "$\\Rightarrow $\n",
    "$\\nabla_{Q} f (P,Q)  = -  ( \\mathbb{1}_{K} \t\\circ (R - QP)) P^{t}  + \\rho Q $\n",
    "\n",
    "\n",
    "$\\nabla f(P,Q) = [\\nabla_{P} f (P,Q),\\nabla_{Q} f (P,Q)]^T$\n",
    "\n",
    "$\\quad \\quad \\quad \\quad= [-  Q^{t} ( \\mathbb{1}_{K} \t\\circ (R - QP))  + \\rho P,-  ( \\mathbb{1}_{K} \t\\circ (R - QP)) P^{t}  + \\rho Q]^T $\n",
    "\n",
    "**1.3.3. Gradient Lipschitzien ? **                       \n",
    "\n",
    "La lipschitziennité du gradient de f se traduit par l'existence d'un réel $L$ vérifiant $\\forall (P1,Q1) \\forall (P2,Q2)$ l'inégalité : \n",
    "\n",
    "\n",
    "$|| \\nabla f(P1,Q1)- \\nabla f(P1,Q1) ||_{F} \\quad \\le L ||\\begin{pmatrix} \n",
    " P1 - P2 \\\\ \n",
    " Q1 - Q2  \n",
    " \\end{pmatrix} ||_{F} $ \n",
    " \n",
    " or, si on considère le cas scalaire, $\\nabla f(p,q) = [\\quad q²p + \\rho p - rq\\quad ,\\quad p²q - rq +\\rho q \\quad]^T$\n",
    " \n",
    " On peut remarquer que le gradient s'écrit donc comme un polynome de degrès 3 en p et q et n'est donc pas majorable par un polynome de degrés 1 en p et q.\n",
    " \n",
    " $Hess f(p,q) =\n",
    "  \\begin{bmatrix}\n",
    "    q²+\\rho & 2pq-r \\\\\n",
    "    2pq-r & p^2+\\rho\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    " $||Hess f(p,q)|| = \\sqrt { (q²+\\rho)^2 + (2pq-r)^2 + (2pq-r)^2 + (p^2+\\rho)^2}$ n'est pas bornée\n",
    " \n",
    " en effet, tous les coefficients sont des polynômes en p et q qui ne sont pas bornés.\n",
    "\n",
    " On en déduit que $\\nabla f$ n'est pas Lipschitzienne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trouver P quand $Q_0$ est fixé\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "\n",
    "\n",
    "$g(P) = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-Q^0 P)||_{F}^2 + \\frac{\\rho}{2}||Q^0||_{F}^2+ \\frac{\\rho}{2}||P||_{F}^2 $\n",
    "\n",
    "$P^{1} = argmin_{P} g(P)$\n",
    "\n",
    "**2.1.1.  Convexité ** \n",
    "\n",
    "** convexité : **\n",
    "\n",
    "$g(P) = f(P,Q_0) = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-Q_0P)||^2 + \\frac{\\rho}{2}||Q_0||^2+ \\frac{\\rho}{2}||P||^2 $\n",
    "\n",
    "$g(tP_1+(1-t)P_2) = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-Q_0(tP_1+(1-t)P_2))||^2 + \\frac{\\rho}{2}||Q_0||^2+ \\frac{\\rho}{2}||tP_1+(1-t)P_2||^2$\n",
    "\n",
    "$\\quad = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ((tR+(1-t)R)-Q_0(tP_1+(1-t)P_2))||^2 + \\frac{\\rho}{2}||Q_0||^2+ \\frac{\\rho}{2}||tP_1+(1-t)P_2||^2$\n",
    "\n",
    "$\\quad = \\frac{1}{2} ||(\\mathbb{1}_{K}\\circ(tR-tQ_0P_1))+(\\mathbb{1}_{K}\\circ((1-t)R-(1-t)Q_0P_2))||^2 + \\frac{\\rho}{2}||Q_0||^2+ \\frac{\\rho}{2}||tP_1+(1-t)P_2||^2$\n",
    "\n",
    "$\\quad \\le \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(tR-tQ_0P_1)||^2 + \\frac{1}{2} ||\\mathbb{1}_{K}\\circ((1-t)R-(1-t)Q_0P_2)||^2 + \\frac{\\rho}{2}||Q_0||^2+ \\frac{\\rho}{2}||tP_1||^2 + \\frac{\\rho}{2}||(1-t)P_2||^2$\n",
    "\n",
    "$\\quad \\le \\frac{1}{2}t||\\mathbb{1}_{K}\\circ(R-Q_0P_1)||^2 + \\frac{1}{2} (1-t)||\\mathbb{1}_{K}\\circ(R-Q_0P_2)||^2 + \\frac{\\rho}{2}||Q_0||^2 + \\frac{\\rho}{2}t||P_1||^2 + \\frac{\\rho}{2}(1-t)||P_2||^2$\n",
    "\n",
    "$\\quad \\le tg(P_1)+(1-t)g(P_2) $\n",
    "\n",
    "Donc, g(P) est convexe.\n",
    "\n",
    "\n",
    "**2.1.1.  Gradient de la fonction g ** \n",
    "\n",
    "Le gradient de g est celui de f quand Q est fixé à $Q^{0}$: \n",
    "\n",
    "\n",
    "$\\nabla g(P) = -  (Q^0 )^{t} ( \\mathbb{1}_{K} \t\\circ (R - Q^0 P))  + \\rho P $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R\n",
    "rho = 0.3\n",
    "#c = 4\n",
    "Q0,sing,P0 = la.svds(R,k=4)\n",
    "del sing\n",
    "L0 = rho + np.sum(((np.transpose(Q0)).dot(Q0))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Voir le code de la fonction objective dans movielensutils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P0\n",
      "[[  1.62060750e-02   3.23391424e-03   4.88005961e-02 ...,  -9.53260775e-04\n",
      "    7.73342642e-05   1.74971250e-03]\n",
      " [ -1.69737618e-02  -6.25039193e-02  -1.16405039e-02 ...,   5.33024145e-04\n",
      "   -4.54336533e-04  -2.61400068e-04]\n",
      " [ -8.72397853e-02  -7.02505798e-03  -2.86181725e-02 ...,  -4.48134760e-04\n",
      "    1.05231342e-04   2.03151884e-04]\n",
      " [  9.59509371e-02   3.51795155e-02   1.99288117e-02 ...,   3.03747116e-05\n",
      "    3.31055915e-04   3.16852950e-04]]\n",
      "Q0\n",
      "[[ 0.08434724 -0.00613256  0.00597506  0.06580431]\n",
      " [-0.01628247  0.05257856 -0.04662602  0.01402104]\n",
      " [-0.02856416  0.02336183 -0.02561845  0.00565798]\n",
      " ..., \n",
      " [ 0.01406097  0.00616532 -0.02502129  0.00744452]\n",
      " [-0.07353537  0.02288736  0.00809611  0.02403119]\n",
      " [ 0.03503116 -0.05854604 -0.01092715  0.04224209]]\n",
      "grad_P0\n",
      "[[ -2.55511347e+00  -5.13441086e-01  -7.73878335e+00 ...,   1.51419184e-01\n",
      "   -1.22846636e-02  -2.77930288e-01]\n",
      " [  3.66873174e+00   1.35718942e+01   2.52859733e+00 ...,  -1.15949951e-01\n",
      "    9.88378905e-02   5.68629500e-02]\n",
      " [  2.12714964e+01   1.71114543e+00   6.98904993e+00 ...,   1.09578207e-01\n",
      "   -2.57325476e-02  -4.96748331e-02]\n",
      " [ -6.13606989e+01  -2.24992343e+01  -1.27481888e+01 ...,  -1.94487024e-02\n",
      "   -2.11983433e-01  -2.02878594e-01]]\n",
      "g(P0)= 685091.782648\n"
     ]
    }
   ],
   "source": [
    "# Requires the objective function defined in movielensutils.py\n",
    "\n",
    "mask\n",
    "R\n",
    "Q0\n",
    "P0\n",
    "print('P0')\n",
    "print(P0)\n",
    "print('Q0')\n",
    "print(Q0)\n",
    "val0, grad_P0 = mlu.objective(P0, Q0, R, mask, 0.3)\n",
    "print('grad_P0')\n",
    "print(grad_P0)\n",
    "print('g(P0)= %f' % val0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** check_grad** : pour vérifier le calcul du gradient de g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erreur : 1.117619 \n",
      "erreur relative: 0.001520 \n"
     ]
    }
   ],
   "source": [
    "#verifies the calculated value of the gradient at P0\n",
    "#takes time to run\n",
    "\n",
    "from numpy import ravel\n",
    "from numpy import reshape\n",
    "\n",
    "def func_g(vectP):\n",
    "    P = reshape(vectP, (4, 1682))\n",
    "    return mlu.objective(P, Q0, R, mask, 0.3)[0]\n",
    "def grad_g(vectP):\n",
    "    P = reshape(vectP, (4, 1682))\n",
    "    return ravel(mlu.objective(P, Q0, R, mask, 0.3)[1])\n",
    "\n",
    "from scipy.optimize import check_grad\n",
    "erreur = check_grad(func_g,grad_g,ravel(P0));\n",
    "print(\"erreur : %f \" % erreur)\n",
    "print(\"erreur relative: %f \" % (erreur / np.linalg.norm(mlu.objective(P0, Q0, R, mask, rho)[1]) ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dernier résultat nous permet de confirmer le calcul et l'implémentation de $\\nabla g(P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "L'algorithme du gradient est basé sur: \n",
    "- $P \\leftarrow P_0$\n",
    "- faire : \n",
    " - $\\quad P_{k+1} \\leftarrow P_k - \\gamma\\nabla g (P_k) \\quad$ où $\\gamma$ est le pas ici pris constant\n",
    "- jusqu'à ce que $||\\nabla g(P_k )||^2_F ≤ \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q0\n",
    "rho\n",
    "R\n",
    "mask\n",
    "def gradient(g, P0, gamma, epsilon):\n",
    "    iterations = 0\n",
    "    P = P0\n",
    "    val,grad_P = g(P, Q0, R, mask, rho)\n",
    "    while True:\n",
    "        iterations = iterations + 1\n",
    "        P = P - gamma*grad_P \n",
    "        val,grad_P = g(P, Q0, R, mask, rho)\n",
    "        if ( ( (np.sum(grad_P ** 2))**(0.5) ) <= epsilon):\n",
    "            break\n",
    "    return P,val,iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'itérations effectuées : 50\n",
      "Le temps de calcul pour l'algorithme du gradient à pas fixe: \n",
      " 854.984045ms \n",
      "La valeur optimale obtenue avec l'algorithme du gradient à pas fixe: \n",
      " 369551.700151\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1\n",
    "P0\n",
    "L0\n",
    "g = mlu.objective #for now g is the defined objective function g(P)\n",
    "start = time()\n",
    "P_opt, val_opt, it = gradient(g,P0, 1/L0, 1) #L0 is the calculated Lipschitz constant\n",
    "Runtime_gradient_pas_fixe = (time() - start) * 1000 # runtime in millis\n",
    "\n",
    "print(\"Nombre d'itérations effectuées : %d\" % it)\n",
    "print(\"Le temps de calcul pour l'algorithme du gradient à pas fixe: \\n %fms \" % Runtime_gradient_pas_fixe)\n",
    "print(\"La valeur optimale obtenue avec l'algorithme du gradient à pas fixe: \\n %f\" % val_opt)\n",
    "#print('la matrice P qui donne cette valeur optimale est: ' )\n",
    "#print(P_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Raffinements algorithmiques pour le problème à $Q_0$ fixé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "** Gradient à Pas Optimal: Armijo's Line Search **\n",
    "\n",
    "Parmi les faiblesses de la méthode de gradient à pas fixe, on note la determination de la constante de Lipschitz elle même et que même si la fonction a un gradient Lipschitzien; l'estimation de la constante de Lipschitz peut prendre en compte les régions où la courbure est grande mais qui ne sont jamais atteintes par l'algorithme. Appliquer une méthode de recherche linéaire peut donc résoudre ce problème et donc choisir un pas $\\gamma_{k} $  plus adapté en utilisant des informations locales.\n",
    "\n",
    "L'idée de la méthode Linaire de Armijo est, étant donnée a ∈ (0, 1),b > 0 et β ∈ (0, 1), on veut determiner le meilleur entier **l** vérifiant l'inégalité suivante: \n",
    "\n",
    "$\\quad f(x^+ (ba^l)) ≤ f(x_{k}) + β <∇f(x_{k}), x^+ ba^l − x_{k}> $\n",
    "\n",
    "avec: $\\gamma_{k} =  ba^l $\n",
    "\n",
    "et $ \\quad x^+ (\\gamma_{k}) = x^+ (ba^l) = x_{k} - \\gamma_{k} \\nabla f (x_{k})$ \n",
    "\n",
    "ici on prend f = g et x= P, on aura alors: \n",
    "\n",
    "$\\quad g(P^+ (ba^l)) ≤ g(x_{k}) + β <∇g(P_{k}), P^+ ba^l − P_{k}> $\n",
    "\n",
    "$\\quad \\quad  \\quad  \\quad \\quad  \\leq  g(x_{k})  + β <∇g(P_{k}), P_ {k} - ba^l ∇g(P_{k}) − P_{k}>  $\n",
    "\n",
    "$\\quad \\quad  \\quad  \\quad \\quad  \\leq  g(x_{k})  + β <∇g(P_{k}), - ba^l ∇g(P_{k}) >  $\n",
    "\n",
    "$\\quad \\quad  \\quad  \\quad \\quad  \\leq  g(x_{k})  + β tr[(∇g(P_{k}))^{t} (P_ {k} - ba^l ∇g(P_{k}) − P_{k})] $\n",
    "\n",
    "Pour cela, on designe par **g_Pplus** le terme à gauche de l'inégalité donné par : $ g_{P} plus = g(P^+ (ba^l)) ≤ g(x_{k}) + β $\n",
    "\n",
    "et par ** ps**  le produit scalaire tel que: $ps =  <∇g(P_{k}), - ba^l ∇g(P_{k}) > $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'itérations effectuées : 7\n",
      "Temps d'execution de l'algorithme avec recherche linéaire: \n",
      " 557.163000 ms\n",
      "valeur retrouvé par l'algorithme avec recherche linéaire: \n",
      " 369551.060504\n"
     ]
    }
   ],
   "source": [
    "Q0\n",
    "rho\n",
    "R\n",
    "mask\n",
    "def gradient_armijo(g, P0, epsilon):\n",
    "    def armijo(a,b,beta,Pk):\n",
    "        l = 0\n",
    "        while True:\n",
    "            g_Pk, grad_Pk = g(Pk, Q0, R, mask, rho)\n",
    "            g_Pplus = g(Pk-b*(a**l)*grad_Pk , Q0, R, mask, rho)[0]\n",
    "            ps = np.trace((np.transpose(grad_Pk)).dot(-b*(a**l)*grad_Pk))\n",
    "            if ( g_Pplus <= ( g_Pk+beta*ps ) ):\n",
    "                break\n",
    "            l = l+1    \n",
    "        return b*(a**l) #returns the determined gamma \n",
    "    \n",
    "    P = P0\n",
    "    val,grad_P = g(P, Q0, R, mask, rho)\n",
    "    a = 0.5\n",
    "    b = 1\n",
    "    beta = 0.5\n",
    "    it = 0\n",
    "    while True:\n",
    "        gamma = armijo(a,b,beta,P)\n",
    "        P = P - gamma*grad_P \n",
    "        val,grad_P = g(P, Q0, R, mask, rho)\n",
    "        it = it +1\n",
    "        if np.sum(grad_P ** 2) <= epsilon:\n",
    "            break\n",
    "        b = 2 * gamma\n",
    "    return P,val,it \n",
    "\n",
    "g\n",
    "start = time()\n",
    "P_armijo, val_armijo,it = gradient_armijo(g,P0, 1)\n",
    "Runtime_gradient_armijo = (time() - start) * 1000 # time in millis\n",
    "\n",
    "print(\"Nombre d'itérations effectuées : %d\" % it)\n",
    "print(\"Temps d'execution de l'algorithme avec recherche linéaire: \\n %f ms\" % Runtime_gradient_armijo)\n",
    "print(\"valeur retrouvé par l'algorithme avec recherche linéaire: \\n %f\" % val_armijo)\n",
    "#print('P_opt_armijo= ' )\n",
    "#print(P_armijo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "\n",
    "**Gradient Conjugué (Cas d'une fonction quadratique) **\n",
    "\n",
    "En se référant au paragraphe III.8.1 du cours de MDI210, On pourrait appliquer la méthode du gradient conjugué pour une fonction quadratique dont la matrice est symetrique, définie, positive.\n",
    "\n",
    "$ g(P) = \\frac{1}{2}\\left|\\left|\n",
    "\\mathbb{1}_K \\circ (R - Q_0P) \\right|\\right|_F^2 + \\frac{\\rho}{2}\\left|\\left|\n",
    "Q_0\\right|\\right|_F^2 + \\frac{\\rho}{2}\\|\n",
    "P \\|_F^2 = \\frac{1}{2} \\left( \\left(\\mathbb{1}_K \\circ (R - Q_0P)\\right)^T\\mathbb{1}_K \\circ(R - Q_0P)\\right) + \\frac{\\rho}{2}\\left( {Q_0}^TQ_0 \\right) + \\frac{\\rho}{2}\\left( P^TP \\right) = \\frac{1}{2} P^T \\left( {Q_0}^T \\mathbb{1}_K \\mathbb{1}_K^T {Q_0} + \\rho I\\right) P - \\frac{1}{2} \\left( P^T{Q_0}^T(\\mathbb{1}_K \\circ R) + (\\mathbb{1}_K \\circ R)^TQP \\right) + \\frac{1}{2}\\left( (\\mathbb{1}_K \\circ R)^T(\\mathbb{1}_K \\circ R) + \\rho {Q_0}^TQ_0\\right)$\n",
    "\n",
    "$\\Rightarrow \\quad g(P) = \\frac{1}{2}P^TAP + b^TP + P^Tb + c$ \n",
    "\n",
    "et\n",
    "$$\\begin{align*} \n",
    "A &= {Q_0}^T (\\mathbb{1}_K \\mathbb{1}_K^T){Q_0} + \\rho I \\\\ \n",
    "b &= -\\frac{1}{2} {Q_0}^T(\\mathbb{1}_K \\circ R)\n",
    "\\\\ c &= \\frac{1}{2}\\left( (\\mathbb{1}_K \\circ R)^T(\\mathbb{1}_K \\circ R) + \\rho {Q_0}^TQ_0\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "A est bien une matrice symetrique, definie, positive. \n",
    "\n",
    "Donc, on peut utiliser l'algorithme du gradient conjugué.\n",
    "\n",
    "**Gradient Conjugué (Cas d'une fonction quelconque) **\n",
    " \n",
    "Pour s'affranchir de la determination des paramètres de cette fonction quadratique, nous proposons d'utiliser la méthode du gradient conjugué pour une fonction quelconque, présentée notamment dans le cours de MDI210 paragraphe III.8.2.\n",
    "Le principe de l'algorithme est le suivant:\n",
    "\n",
    "- partir d'un point $P_0$ ;\n",
    "- faire $\\quad d_0 \\leftarrow −\\nabla g_(P_0 ) \\quad et \\quad k \\leftarrow 0$ ;\n",
    "- répéter\n",
    " - choisir $s_k$ minimisant $g(P_k + sd_k )$, par rapport à s\n",
    " - $P_{k+1} \\leftarrow P_k + s_kd_k$\n",
    " - $b_k \\leftarrow \\frac{||\\nabla g(P_{k+1})||^2}{||\\nabla g(P_{k})||^2}$\n",
    " - $d_{k+1} \\leftarrow −\\nabla g(P_{k+1}) + b_kd_k$\n",
    " - $k \\leftarrow k+1$\n",
    "- jusqu'à ce qu'un test d'arrêt soit vérifié. \n",
    "\n",
    "Ici, On prendra comme condition d'arrêt: $||\\nabla g(P_k)|| \\le \\epsilon$\n",
    "\n",
    "L'autre difficulté dans l'algorithme est la determination de s qui minimise $g(P_k + sd_k )$. Pour cela on considérera l'expression analytique de $g(P_k + sd_k )$\n",
    "\n",
    "$g(P_k + sd_k) $\n",
    "\n",
    "$\\quad = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-(Q_k+sd_k)P_0)||^2 + \\frac{\\rho}{2}||Q_k+sd_k||^2+ \\frac{\\rho}{2}||P_0||^2$\n",
    "\n",
    "$\\quad = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-(Q_kP_0)||^2 + \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(sd_kP_0)||^2 - <\\mathbb{1}_{K}\\circ(R-(Q_kP_0), \\mathbb{1}_{K}\\circ(sd_kP_0)> + \\frac{\\rho}{2}||Q_k||^2 + \\frac{\\rho}{2}||sd_k||^2 + \\rho <Q_k, sd_k> + \\frac{\\rho}{2}||P_0||^2$\n",
    "\n",
    "$\\quad = \\frac{1}{2} ||\\mathbb{1}_{K}\\circ(R-(Q_kP_0)||^2 + \\frac{1}{2} s^2||\\mathbb{1}_{K}\\circ(d_kP_0)||^2 - s<\\mathbb{1}_{K}\\circ(R-(Q_kP_0), \\mathbb{1}_{K}\\circ(d_kP_0)> + \\frac{\\rho}{2}||Q_k||^2 + \\frac{\\rho}{2}s^2||d_k||^2 + \\rho <Q_k, sd_k> + \\frac{\\rho}{2}||P_0||^2$\n",
    "\n",
    "$\\frac{\\partial g(P_k + sd_k)}{\\partial s} = s||\\mathbb{1}_{K}\\circ(d_kP_0)||^2 - <\\mathbb{1}_{K}\\circ(R-(Q_kP_0), \\mathbb{1}_{K}\\circ(d_kP_0)> + \\rho s||d_k||^2 + \\rho <Q_k, d_k> $\n",
    "\n",
    "En écrivant $\\frac{\\partial g(P_k + sd_k)}{\\partial s} = 0$, On retrouve:\n",
    "\n",
    "$s_{min} = \\frac{<\\mathbb{1}_{K}\\circ(R-(Q_kP_0), \\mathbb{1}_{K}\\circ(d_kP_0)> - \\rho <Q_k, d_k>}{||\\mathbb{1}_{K}\\circ(d_kP_0)||^2 + \\rho ||d_k||^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'itérations effectuées : 6\n",
      "Le temps d'execution de l'algorithme du gradient conjugué : \n",
      " 2339.318037ms\n",
      "valeur optimale par l'algorithme du gradient conjugué : \n",
      " 369550.880233\n"
     ]
    }
   ],
   "source": [
    "def scal(A,B):\n",
    "    return ( np.trace((np.transpose(A)).dot(B)) )\n",
    "\n",
    "def s_min(Pk, dk, Q0, R, mask, rho ):\n",
    "    num = -rho*scal(Pk,dk) - scal( (mask*(Q0.dot(Pk)-R) ) , (mask*(Q0.dot(dk)) ) )\n",
    "    denom = rho * scal(dk,dk) + scal(mask*(Q0.dot(dk)),mask*(Q0.dot(dk)) )\n",
    "    s = num / denom\n",
    "    return s\n",
    "\n",
    "epsilon = 1\n",
    "R\n",
    "mask\n",
    "rho\n",
    "\n",
    "def gradient_conjugue (g,P0, Q0, R, mask, rho, epsilon):\n",
    "    Q0    \n",
    "    P0\n",
    "    dk =  -g(P0, Q0, R, mask, rho)[1] #initialisation \n",
    "    k = 0\n",
    "    Pk = P0\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations = iterations + 1\n",
    "        s = s_min(Pk, dk, Q0, R, mask, rho)\n",
    "        Pk1 = Pk + s*dk\n",
    "        grad_Pk = g(Pk, Q0, R, mask, rho)[1]\n",
    "        grad_Pk1 = g(Pk1, Q0, R, mask, rho)[1]\n",
    "        bk = scal(grad_Pk1,grad_Pk1)/ scal(grad_Pk,grad_Pk)\n",
    "        dk = -grad_Pk1 + bk*dk # mise à jour de dk1\n",
    "        Pk = Pk1\n",
    "        if (  np.sum(grad_Pk1 ** 2) <= epsilon ):\n",
    "            break\n",
    "    return Pk, iterations\n",
    "\n",
    "start = time()\n",
    "Pkmin, it = gradient_conjugue (g,P0, Q0, R, mask, rho, epsilon)\n",
    "Runtime_gradient_conjugue = (time() - start) * 1000 # time in millis\n",
    "\n",
    "print(\"Nombre d'itérations effectuées : %d\" % it)\n",
    "print(\"Le temps d'execution de l'algorithme du gradient conjugué : \\n %fms\" % Runtime_gradient_conjugue)\n",
    "print(\"valeur optimale par l'algorithme du gradient conjugué : \\n %f\" % g(Pkmin, Q0, R, mask, rho)[0])\n",
    "#print(\"P_opt_gradientConjugué : \")\n",
    "#print(Pkmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "En vue de comparer la rapidité des algorithmes, on peut appliquer l'un des deux critères suivants:\n",
    "\n",
    " - **Critère 1**: **Runtime**\n",
    " \n",
    "On utilise la fonction time.time() pour estimer les temps de calculs des algorithmes implémentés.\n",
    "\n",
    "Les **Runtime** de chacuns des trois algorithmes sont comme suit: \n",
    " \n",
    "   **$\\diamond $ Gradient à pas constant**:  839.606047ms\n",
    "   \n",
    "   **$\\diamond $ Méthode de Recherche Linéaire de Armijo**:  539.134026 ms\n",
    "   \n",
    "   **$\\diamond $ Gradient Conjugué **: 2165.952921ms\n",
    "\n",
    "$\\Rightarrow $ Donc on peut dire  que la méthode de recherche linéaire de Armijo est la plus rapide car elle nous permet d'eviter les pas trés grands qui augmentent le nombre d'itérations comme cet algorithme consiste à reduire progressivement la valeur du paramètre d'accélération jusqu'à en trouver une qui satisfait la condition d'Armijo.\n",
    "L'algorithme du gradient conjugué est le plus lent cela peut être dû à l'implémentation choisie qui n'utilise pas le fait que g est quadratique et qui implémente un algorithme générique faisant intervenir des calculs supplémentaires. \n",
    " \n",
    " - **Critère 2**: **Comptage des itérations **\n",
    "\n",
    "Les **nombres d'itérations ** de chacuns des trois algorithmes sont comme suit: \n",
    " \n",
    "   **$\\diamond $ Gradient à pas constant**:  50\n",
    "   \n",
    "   **$\\diamond $ Méthode de Recherche Linéaire de Armijo**:  7\n",
    "   \n",
    "   **$\\diamond $ Gradient Conjugué **: 6\n",
    "\n",
    "$\\Rightarrow $ Contrairement au classement fournit par le critère précédent, on voit qu'ici c'est la méthode du gradient conjugué qui offre une convergence plus rapide et proche à la recherche linéaire. Cependant, Le gradient à pas constant nécessite un grand nombre d'itérations ce qui est expliqué par le fait même que le pas soit constant et que l'algorithme doit tourner plusieurs fois pour avancer dans une direction données contrairement au deux autres algorithmes qui optimise la longueur du pas effectué."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Résolution du problème complet\n",
    "### Question 4.1\n",
    "\n",
    "L'algorithme prend beaucoup de temps pour tourner. C'est pour cela qu'il y a deux alternative dans le code.\n",
    "La première alternative \"alternative 1\" n'est pas précise mais ne prend qu'environ 90 secondes.\n",
    "Si vous avez le temps commentez l' \"alternative 1\" et décommenter l' \"alternative 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le temps d'execution est : 89.285617 \n",
      "La valeur retrouvé est : \n",
      " 34365.408888\n",
      "Les solutions retrouvées sont :\n",
      "P: \n",
      "[[-0.26905088 -0.10910263  0.6979702  ...,  0.03228779 -0.20605116\n",
      "   0.30560488]\n",
      " [-0.1072123  -0.46977553 -0.19797729 ...,  0.39087079  0.01151933\n",
      "   0.41791422]\n",
      " [-0.19639943 -0.20853861 -0.84937913 ..., -0.62263778  0.25613586\n",
      "   0.08047297]\n",
      " [ 1.93876026  1.60361052  1.34599701 ...,  0.26968794  1.39581177\n",
      "   1.1991189 ]]\n",
      "Q: \n",
      "[[ 0.54378289  0.57471236  0.09489071  2.16749607]\n",
      " [-0.40239689  0.67843883  0.46357677  2.15438001]\n",
      " [ 0.90727517 -0.46432211  0.5966598   2.1105597 ]\n",
      " ..., \n",
      " [-0.03635137  0.73986335 -0.4244401   2.19406521]\n",
      " [-1.22354587 -0.06878356  0.03136357  2.13184373]\n",
      " [ 0.93484468 -0.29639925 -0.73780161  2.02853328]]\n"
     ]
    }
   ],
   "source": [
    "rho\n",
    "R\n",
    "mask\n",
    "\n",
    "def gradientPQ(g, P0, Q0, epsilon):\n",
    "    \n",
    "    def armijoPQ(a,b,beta,Pk,Qk):\n",
    "        l = 0\n",
    "        g_k, grad_Pk, grad_Qk = g(Pk, Qk, R, mask, rho)\n",
    "        while True:\n",
    "            Pplus = Pk-b*(a**l)*grad_Pk\n",
    "            Qplus = Qk-b*(a**l)*grad_Qk\n",
    "            g_plus, grad_Pplus, grad_Qplus = g(Pplus ,Qplus , R, mask, rho)\n",
    "            \n",
    "            ps = np.trace(np.concatenate((grad_Pk,np.transpose(grad_Qk)),axis=1).dot(np.transpose(np.concatenate((Pplus-Pk,np.transpose(Qplus-Qk)),axis=1))))\n",
    "            if ( g_plus <= ( g_k+beta*ps ) ):\n",
    "                break\n",
    "            l = l+1    \n",
    "        return b*(a**l) #returns the determined gamma \n",
    "    \n",
    "    P = P0\n",
    "    Q = Q0\n",
    "    g = mlu.total_objective\n",
    "    val,grad_P,grad_Q = g(P, Q, R, mask, rho)\n",
    "    a = 0.5\n",
    "    b = 1\n",
    "    beta = 0.5\n",
    "    i = 0\n",
    "    while True:\n",
    "        gamma = armijoPQ(a,b,beta,P,Q)\n",
    "        P = P - gamma*grad_P\n",
    "        Q = Q - gamma*grad_Q\n",
    "        val,grad_P,grad_Q = g(P, Q, R, mask, rho)\n",
    "        b = 2 * gamma\n",
    "        \n",
    "        #YOU CAN UNCOMMENT the \"alternative 2\" line if you have time to wait for the code to terminate BUT\n",
    "        #IF YOU DO, then DON'T forget to comment the \"alternative 1\" and to restore the code after you've finished\n",
    "        if  ( ( np.sum(grad_P ** 2) ) < epsilon ): #alternative 1\n",
    "        #if ( np.sum(grad_P ** 2) + np.sum(grad_Q ** 2) ) < epsilon: #alternative 2\n",
    "            break\n",
    "           \n",
    "    return P, Q, val \n",
    "    \n",
    "epsilon = 100\n",
    "g=mlu.total_objective\n",
    "start = time()\n",
    "P_armijo, Q_armijo, val_armijo = gradientPQ(g,P0,Q0, epsilon)\n",
    "Runtime_gradientCroise = time()-start\n",
    "\n",
    "print(\"Le temps d'execution est : %f \" % Runtime_gradientCroise)\n",
    "print(\"La valeur retrouvé est : \\n %f\" % val_armijo)\n",
    "\n",
    "print(\"Les solutions retrouvées sont :\")\n",
    "print(\"P: \")\n",
    "print(P_armijo)\n",
    "print(\"Q: \")\n",
    "print(Q_armijo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est en train de minimiser la fonction f(P,Q) qui n'est pas convexe. Les minimums retrouvés sont, donc, susceptibles d'être des minimums locaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montrons que la valeur de l’objectif décroit à chaque itération pour enfin déduire la convergence.\n",
    "\n",
    "D'abord, l'algorithme de la méthode des moindres carrés alternés est:\n",
    "\n",
    "$\\quad$ Pour $k\\ge1$ faire :\n",
    "\n",
    "$\\qquad P_k\\leftarrow argmin_P f(P, Q_{k-1})$\n",
    "\n",
    "$\\qquad Q_k \\leftarrow argmin_Q f(P_k, Q)$\n",
    "\n",
    "$\\quad$ fin\n",
    "\n",
    "à l'étape k,\n",
    "\n",
    "notons la valeur minimale retrouvée jusque là: $f_{k-1,2} = f(P_{k-1},Q_{k-1})$\n",
    "\n",
    "La première partie de l'étape k est $\\quad P_k \\leftarrow arg min_P f(P, Q_{k-1}) $\n",
    "\n",
    "Donc $f_{k,1} = f(P_k,Q_{k-1}) \\le f(P_{k-1},Q_{k-1}) = f_{k-1,2}$\n",
    "\n",
    "La deuxième partie est $\\quad Q_k \\leftarrow argmin_Q f(P_k, Q)$\n",
    "\n",
    "Donc $f_{k,2} = f(P_k,Q_k) \\le f(P_k,Q_{k-1}) = f_{k,1}$\n",
    "\n",
    "Finalement $f_{k,2} \\le f_{k-1,2}$\n",
    "\n",
    "On remarque bien que la valeur de la fonction objective décroît à chaque nouvelle itération.\n",
    "\n",
    "Si on note $v_k = f_{k,2}$ la valeur de la fonction objective retournée à l'étape k de l'algorithme, On remarque que $v_k$ est une suite décroissante\n",
    "\n",
    "De plus, $v_k$ est bornée car $f(P^*,Q^*)=min_{(P,Q)} f(P,Q) \\le v_k$ et de plus elle prend des valeurs dans $\\mathbb{R_+}$ donc minorée par 0 au pire des cas.\n",
    "\n",
    "Donc $v_k$ converge vers $f(P^*,Q^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation de la méthode des moindres carrés alternés avec l'algorithme de recherche linéaire pour la minimisation par rapport à P et Q.\n",
    "\n",
    "L'***algorithme prend beaucoup de temps pour tourner***. Si vous n'avez pas assez de temps (~25 minutes), commentez la boucle while et utiliser la boucle for (la boucle for prend environ 230secondes) qui est en commentaire. Rétablissez le code quand vous aurez fini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60927.3633929\n",
      "51256.4785339\n",
      "46635.8361022\n",
      "44186.812017\n",
      "42647.7271689\n",
      "41362.7454045\n",
      "40343.4487067\n",
      "39690.6186819\n",
      "39075.1833426\n",
      "38619.5663492\n",
      "38237.1690922\n",
      "37903.5865658\n",
      "37547.3136085\n",
      "37239.9941206\n",
      "37001.3218913\n",
      "36733.4965158\n",
      "36551.2558392\n",
      "36364.2261847\n",
      "36206.5137283\n",
      "36086.0431964\n",
      "35965.4531846\n",
      "35871.7257751\n",
      "35781.7972493\n",
      "35699.4094447\n",
      "35622.42491\n",
      "35541.3314129\n",
      "35468.6098347\n",
      "35400.2080874\n",
      "35332.8389469\n",
      "35265.8504874\n",
      "35201.3643112\n",
      "35147.1952648\n",
      "35101.3491284\n",
      "35050.0781417\n",
      "35002.9657858\n",
      "34959.0587944\n",
      "34911.0091055\n",
      "34878.2262654\n",
      "34854.7318397\n",
      "34824.257885\n",
      "34793.4732999\n",
      "34767.8710286\n",
      "34743.7174995\n",
      "34719.1510414\n",
      "34698.1714615\n",
      "34678.1938744\n",
      "34660.8368751\n",
      "34644.7732738\n",
      "34629.8969562\n",
      "34614.0925442\n",
      "34601.1545683\n",
      "34587.9235674\n",
      "34575.98204\n",
      "34565.1391949\n",
      "34554.4616832\n",
      "34544.4699103\n",
      "34534.974726\n",
      "34525.4315594\n",
      "34516.7862856\n",
      "34508.8201799\n",
      "34500.0539486\n",
      "34492.9253111\n",
      "34485.863383\n",
      "34479.2763564\n",
      "34472.2829565\n",
      "34465.8995734\n",
      "34459.4862142\n",
      "34453.0100197\n",
      "34446.8601779\n",
      "34441.4024715\n",
      "34435.9040164\n",
      "34430.3487474\n",
      "34425.328075\n",
      "34420.0868066\n",
      "34415.4655016\n",
      "34410.8846459\n",
      "34406.5488181\n",
      "Algorithm Terminated Successfully\n",
      "Le temps d'execution est : 1466.954957\n",
      "La valeur optimale est : 34405.434410 \n",
      "Les solutions retrouvées sont :\n",
      "P: \n",
      "[[-0.39813087 -0.25449876  0.88186214 ...,  0.25076266 -0.25796946\n",
      "   0.29890929]\n",
      " [ 0.2322916  -0.23029654 -0.03849453 ...,  0.40216265  0.33405063\n",
      "   0.73774794]\n",
      " [-0.07487866 -0.10068693 -1.10998596 ..., -0.54798769  0.39534682\n",
      "   0.16248363]\n",
      " [ 2.25397443  1.94014176  1.69133752 ...,  0.58157126  1.63759488\n",
      "   1.3076956 ]]\n",
      "Q: \n",
      "[[ 0.35869646  0.64064799  0.36774936  1.77448603]\n",
      " [-0.34429949  0.8495726   0.33409141  1.64497103]\n",
      " [ 0.75968822 -0.02713181  0.46597837  1.67300574]\n",
      " ..., \n",
      " [-0.13781368  0.60721153  0.15447737  1.89407186]\n",
      " [-1.07261269  0.30566531  0.1396019   1.77243028]\n",
      " [ 0.75900026  0.03861954 -0.85783861  1.70438107]]\n"
     ]
    }
   ],
   "source": [
    "g = mlu.total_objective \n",
    "\n",
    "def min_P_armijo(g, P0, Q0, epsilon):\n",
    "    def armijoP(a,b,beta,Pk):\n",
    "        l = 0\n",
    "        while True:\n",
    "            g_Pk, grad_Pk = g(Pk, Q0, R, mask, rho)[0:2]\n",
    "            g_Pplus = g(Pk-b*(a**l)*grad_Pk , Q0, R, mask, rho)[0]\n",
    "            ps = np.trace((np.transpose(grad_Pk)).dot(-b*(a**l)*grad_Pk))\n",
    "            if ( g_Pplus <= ( g_Pk+beta*ps ) ):\n",
    "                break\n",
    "            l = l+1    \n",
    "        return b*(a**l) #returns the determined gamma \n",
    "    \n",
    "    P = P0\n",
    "    val, grad_P = g(P, Q0, R, mask, rho)[0:2]\n",
    "    a = 0.5\n",
    "    b = 1\n",
    "    beta = 0.5\n",
    "    while True:\n",
    "        gamma = armijoP(a,b,beta,P)\n",
    "        P = P - gamma*grad_P \n",
    "        val,grad_P = g(P, Q0, R, mask, rho)[0:2]\n",
    "        if np.sum(grad_P ** 2) <= epsilon:\n",
    "            break\n",
    "        b = 2 * gamma\n",
    "    return P,val\n",
    "\n",
    "def min_Q_armijo(g, P0, Q0, epsilon):\n",
    "    def armijoQ(a,b,beta,Qk):\n",
    "        l = 0\n",
    "        while True:\n",
    "            g_Qk, grad_Pk, grad_Qk = g(P0, Qk, R, mask, rho)\n",
    "            del grad_Pk\n",
    "            g_Qplus = g(P0, Qk-b*(a**l)*grad_Qk , R, mask, rho)[0]\n",
    "            ps = np.trace((np.transpose(grad_Qk)).dot(-b*(a**l)*grad_Qk))\n",
    "            if ( g_Qplus <= ( g_Qk+beta*ps ) ):\n",
    "                break\n",
    "            l = l+1    \n",
    "        return b*(a**l) #returns the determined gamma \n",
    "    \n",
    "    P0\n",
    "    Q = Q0\n",
    "    val,grad_P,grad_Q = g(P0, Q, R, mask, rho)\n",
    "    a = 0.5\n",
    "    b = 1\n",
    "    beta = 0.5\n",
    "    while True:\n",
    "        gamma = armijoQ(a,b,beta,Q)\n",
    "        Q = Q - gamma*grad_Q \n",
    "        val,grad_P, grad_Q = g(P0, Q, R, mask, rho)\n",
    "        if np.sum(grad_Q ** 2) <= epsilon:\n",
    "            break\n",
    "        b = 2 * gamma\n",
    "    return Q,val\n",
    "\n",
    "pmin = P0\n",
    "qmin= Q0\n",
    "start = time()\n",
    "\n",
    "#for i in range(0,200): #UNCOMMENT THIS AND COMMENT NEXT LINE, if you can't wait\n",
    "it = 0\n",
    "while True: \n",
    "    it = it +1\n",
    "    pmin = min_P_armijo(g,pmin,qmin,epsilon)[0]\n",
    "    qmin = min_Q_armijo(g,pmin,qmin,10**7)[0] #minimizing over q is very time consuming\n",
    "    val, gradP, gradQ = mlu.total_objective(pmin, qmin, R, mask, rho)\n",
    "    if np.sum(gradQ ** 2) <= epsilon:\n",
    "        if np.sum(gradP ** 2) <= epsilon:\n",
    "            print(\"Algorithm Terminated Successfully\")\n",
    "            break;\n",
    "            \n",
    "    #Cela permet l'affichage des valeurs chaque 40 itérations pour vérifier la décroissance\n",
    "    if ( (it%40) == 0 ):\n",
    "        print(val)\n",
    "            \n",
    "runtime_alterne_armijo = time()- start\n",
    "        \n",
    "print(\"Le temps d'execution est : %f\" % runtime_alterne_armijo)\n",
    "print(\"La valeur optimale est : %f \" % g(pmin, qmin, R, mask, rho)[0])\n",
    "print(\"Les solutions retrouvées sont :\")\n",
    "print(\"P: \")\n",
    "print(pmin)\n",
    "print(\"Q: \")\n",
    "print(qmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparaison entre les résultats:\n",
    "\n",
    "La comparaison effectuée ici n'est pas pertinente car on a forcé dans l'implémentation un arrêt prématuré des algorithmes vu qu'ils prennent beaucoup de temps.\n",
    "\n",
    "- ** Les solutions **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les solutions retrouvées par la méthode du gradient avec recherche linéaire :\n",
      "P: \n",
      "[[-0.26905088 -0.10910263  0.6979702  ...,  0.03228779 -0.20605116\n",
      "   0.30560488]\n",
      " [-0.1072123  -0.46977553 -0.19797729 ...,  0.39087079  0.01151933\n",
      "   0.41791422]\n",
      " [-0.19639943 -0.20853861 -0.84937913 ..., -0.62263778  0.25613586\n",
      "   0.08047297]\n",
      " [ 1.93876026  1.60361052  1.34599701 ...,  0.26968794  1.39581177\n",
      "   1.1991189 ]]\n",
      "Q: \n",
      "[[ 0.54378289  0.57471236  0.09489071  2.16749607]\n",
      " [-0.40239689  0.67843883  0.46357677  2.15438001]\n",
      " [ 0.90727517 -0.46432211  0.5966598   2.1105597 ]\n",
      " ..., \n",
      " [-0.03635137  0.73986335 -0.4244401   2.19406521]\n",
      " [-1.22354587 -0.06878356  0.03136357  2.13184373]\n",
      " [ 0.93484468 -0.29639925 -0.73780161  2.02853328]]\n",
      "Les solutions retrouvées par les moindres carrés alternés :\n",
      "P: \n",
      "[[-0.39813087 -0.25449876  0.88186214 ...,  0.25076266 -0.25796946\n",
      "   0.29890929]\n",
      " [ 0.2322916  -0.23029654 -0.03849453 ...,  0.40216265  0.33405063\n",
      "   0.73774794]\n",
      " [-0.07487866 -0.10068693 -1.10998596 ..., -0.54798769  0.39534682\n",
      "   0.16248363]\n",
      " [ 2.25397443  1.94014176  1.69133752 ...,  0.58157126  1.63759488\n",
      "   1.3076956 ]]\n",
      "Q: \n",
      "[[ 0.35869646  0.64064799  0.36774936  1.77448603]\n",
      " [-0.34429949  0.8495726   0.33409141  1.64497103]\n",
      " [ 0.75968822 -0.02713181  0.46597837  1.67300574]\n",
      " ..., \n",
      " [-0.13781368  0.60721153  0.15447737  1.89407186]\n",
      " [-1.07261269  0.30566531  0.1396019   1.77243028]\n",
      " [ 0.75900026  0.03861954 -0.85783861  1.70438107]]\n",
      "distance entre les P : 29.213834 \n",
      "distance entre les Q : 21.116059 \n"
     ]
    }
   ],
   "source": [
    "print(\"Les solutions retrouvées par la méthode du gradient avec recherche linéaire :\")\n",
    "print(\"P: \")\n",
    "print(P_armijo)\n",
    "print(\"Q: \")\n",
    "print(Q_armijo)\n",
    "\n",
    "print(\"Les solutions retrouvées par les moindres carrés alternés :\")\n",
    "print(\"P: \")\n",
    "print(pmin)\n",
    "print(\"Q: \")\n",
    "print(qmin)\n",
    "\n",
    "from math import sqrt\n",
    "print(\"distance entre les P : %f \" % sqrt(np.sum( (P_armijo-pmin)**2 )) )\n",
    "print(\"distance entre les Q : %f \" % sqrt(np.sum( (Q_armijo-qmin)**2 )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - ** La prédiction R  = QP **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la prédiction de R avec la méthode du gradient avec recherche linéaire\n",
      "[[ 3.97569727  3.12671719  3.1026093  ...,  0.76766082  2.94429466\n",
      "   3.0130848 ]\n",
      " [ 4.1213084   3.08330141  2.0908601  ...,  0.5445593   3.21657713\n",
      "   2.78121796]\n",
      " [ 3.7803635   3.3792302   3.05919292 ...,  0.04549351  2.90647627\n",
      "   2.66204791]\n",
      " ..., \n",
      " [ 4.26758415  3.26333454  3.14186747 ...,  1.14400263  2.96980069\n",
      "   2.89487935]\n",
      " [ 4.46354467  3.57791143  2.00243475 ...,  0.48901332  3.23500661\n",
      "   2.1561908 ]\n",
      " [ 3.85800039  3.44408453  4.06824706 ...,  0.92078436  2.44643304\n",
      "   2.53490313]]\n",
      "la prédiction de R avec les moindres carrés altérenés\n",
      "[[ 3.97811857  3.16690008  2.88471756 ...,  1.17806033  3.17275393\n",
      "   2.96009527]\n",
      " [ 4.01713116  3.04980852  2.07503587 ...,  1.02891881  3.19849715\n",
      "   2.72926189]\n",
      " [ 3.42726248  3.01185901  2.98337263 ...,  0.89721168  2.71888894\n",
      "   2.47055753]\n",
      " ..., \n",
      " [ 4.4535405   3.55444876  2.88714012 ...,  1.22652534  3.40118565\n",
      "   2.90874476]\n",
      " [ 4.48260302  3.62729485  1.88515871 ...,  0.80825035  3.33652293\n",
      "   2.24537236]\n",
      " [ 3.61265471  3.19105544  4.50271941 ...,  1.6671643   2.26904395\n",
      "   2.3447906 ]]\n",
      "distance entre les prédictions R : 580.418605 \n"
     ]
    }
   ],
   "source": [
    "#la méthode du gradient avec recherche linéaire (4.1)\n",
    "Rgrl = Q_armijo.dot(P_armijo)\n",
    "print(\"la prédiction de R avec la méthode du gradient avec recherche linéaire\")\n",
    "print(Rgrl)\n",
    "\n",
    "# la méthode des moindres carrés alternés\n",
    "Rmca = qmin.dot(pmin) \n",
    "print(\"la prédiction de R avec les moindres carrés altérenés\")\n",
    "print(Rmca)\n",
    "\n",
    "from math import sqrt\n",
    "print(\"distance entre les prédictions R : %f \" % sqrt(np.sum( (Rmca-Rgrl)**2 )) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** La valeur de la fonction objective **\n",
    "\n",
    "Ici, la valeur retrouvée par la méthode des moindres carrés alternés est moins bonne que celle retrouvée par la méthode du gradient avec recherche linéaire. Cela est dû au fait que l'implémentation executée force un arrêt de la méthode des moindres carrés alternés avec la convergence (en raison du grand temps de calcul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La valeur retrouvé par le gradient avec recherche linéaire : \n",
      " 34365.408888\n",
      "La valeur retrouvé par la méthode des moindres carrés alternés : \n",
      " 34405.434410\n"
     ]
    }
   ],
   "source": [
    "#la méthode du gradient avec recherche linéaire \n",
    "print(\"La valeur retrouvé par le gradient avec recherche linéaire : \\n %f\" % val_armijo)\n",
    "\n",
    "#la méthode des moindres carrés alternés\n",
    "print(\"La valeur retrouvé par la méthode des moindres carrés alternés : \\n %f\" % g(pmin, qmin, R, mask, rho)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Comparaison des temps de calcul **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La méthode des moindres carrés alternés prend plus de temps que la méthode du gradient avec recherche linéaire. Mais, la méthode des moindres carrés alternés fournit (potentiellement) une meilleure solution.\n",
    "\n",
    "En effet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le temps d'execution du gradient avec recherche linéaire est : 89.285617 \n",
      "Le temps d'execution des moindres carrés alternés est : 1466.954957\n"
     ]
    }
   ],
   "source": [
    "print(\"Le temps d'execution du gradient avec recherche linéaire est : %f \" % Runtime_gradientCroise)\n",
    "print(\"Le temps d'execution des moindres carrés alternés est : %f\" % runtime_alterne_armijo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quel ﬁlm recommanderiez-vous à l’utilisateur 300 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit **mask_br** afin d'éliminer les films dont l'utilisateur 300 connait et donc les scores associés. Par suite, on ne laisse que les films dont l'utilisateur ne connait pas le score. Apres on determine le film **best_predicted_item** correspondant au maximum du score **best_rate_predicted_item**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_br = np.full((943,1682),True)\n",
    "mask_br[mask] = False\n",
    "R_unseen = Rgrl*mask_br\n",
    "User300= R_unseen[299,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le meilleur score est : 7.378320 \n"
     ]
    }
   ],
   "source": [
    "best_rate_predicted_item = np.amax(User300)\n",
    "print(\"Le meilleur score est : %f \" % best_rate_predicted_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le film recommendé à l utilisateur 300 est : 1292\n"
     ]
    }
   ],
   "source": [
    "list300 = User300.tolist()\n",
    "best_predicted_item = list300.index(best_rate_predicted_item)\n",
    "print('Le film recommendé à l ''utilisateur 300 est : %d' % best_predicted_item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
